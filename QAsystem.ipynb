{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rdflib\n",
    "import numpy as np\n",
    "from SPARQLWrapper import SPARQLWrapper, JSON"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from six import iteritems\n",
    "from web.embeddings import load_embedding, fetch_conceptnet_numberbatch\n",
    "from web.evaluate import evaluate_similarity, evaluate_similarity_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 3min 49s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#Fetch FastText-wiki 300d 1M\n",
    "we =load_embedding(\"C:\\\\Users\\\\daix8\\\\web_data\\\\embeddings\\\\fastText\\\\wiki-news-300d-1M.vec\\\\wiki-news-300d-1M.vec\", format='word2vec', normalize=True, lower=False, clean_words=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already downloaded, skipping\n",
      "Wall time: 1min 37s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#Fetch ConceptNetNumb\n",
    "wecn = fetch_conceptnet_numberbatch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparql = SPARQLWrapper(\"http://dbpedia.org/sparql\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# isEntityInDatabase Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isEntityInDatabase(s):\n",
    "    return s.startswith(\"http\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MakeQueryString Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeQueryString(obj, filterStringsPredicates, filterStringSubjects, literals):\n",
    "    if obj in literals:\n",
    "        result = \"SELECT distinct ?s ?p WHERE { \" + \"{?s ?p \" + obj + \" . \" + \"} FILTER (\"\n",
    "        for s in filterStringsPredicates:\n",
    "            fsp = \"!regex(str(?p), '\" + s + \"' , 'i') && \"\n",
    "            result += fsp\n",
    "        for s in filterStringSubjects:\n",
    "            fss = \"!regex(str(?s), '\" + s + \"' , 'i') && \"\n",
    "            result += fss\n",
    "        result = result[:result.rindex(\"&&\")] + \")}\"\n",
    "    else:\n",
    "        result = \"SELECT distinct ?s ?p ?j ?k WHERE { \"\n",
    "        incomingQuery = \"{?s ?p <\" + obj + \"> . FILTER (\"\n",
    "        outgoingQuery =  \"{<\" + obj + \"> ?j ?k. FILTER (\"\n",
    "        \n",
    "        for s in filterStringsPredicates:\n",
    "            fsp = \"!regex(str(?p), '\" + s + \"' , 'i') && \"\n",
    "            incomingQuery += fsp\n",
    "            fsj = \"!regex(str(?j), '\" + s + \"' , 'i') && \"\n",
    "            outgoingQuery += fsj\n",
    "            \n",
    "        for s in filterStringSubjects:\n",
    "            fss = \"!regex(str(?s), '\" + s + \"' , 'i') && \"\n",
    "            fss2 = \"!regex(str(?k), '\" + s + \"' , 'i') && \"\n",
    "            incomingQuery += fss\n",
    "            outgoingQuery += fss2\n",
    "        \n",
    "        incomingQuery = incomingQuery[:incomingQuery.rindex(\"&&\")] + \")}\"\n",
    "        outgoingQuery = outgoingQuery[:outgoingQuery.rindex(\"&&\")] + \")}\"\n",
    "        result = result + incomingQuery + \" UNION \" + outgoingQuery + \"}\"\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Triple Class "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Triple:\n",
    "    def __init__(self):\n",
    "        self.subject = None\n",
    "        self.object = None\n",
    "        self.predicate = None\n",
    "        self.cost = None\n",
    "        self.previousTriple = None\n",
    "        #the list of connecting seeds\n",
    "        self.seeds = []\n",
    "    \n",
    "    def __str__(self):\n",
    "        if(self.subject is not None and self.predicate is not None and self.object is not None):\n",
    "            # print connecting seeds\n",
    "            connectSeeds = \"[\"\n",
    "            for seed in self.seeds:\n",
    "                connectSeeds += seed + \", \"\n",
    "            connectSeeds = connectSeeds[:connectSeeds.rindex(',')] + \"]\"\n",
    "            return self.subject + \"  --  \" + self.predicate + \"  --  \" + self.object + \"  \" + connectSeeds + \" (\" + str(self.cost) + ')'\n",
    "        else:\n",
    "            return \"Not Well Defined Triple\"\n",
    "        \n",
    "    def __eq__(self,other):\n",
    "        if(self.subject == other.getSubject() and self.object == other.getObject() and self.predicate == other.getPredicate()):\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "        \n",
    "    def __gt__(self,other):\n",
    "        if self.cost > other.getCost():\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    \n",
    "    def setSeeds(self, fs):\n",
    "        if fs not in self.seeds:\n",
    "            self.seeds.append(fs)\n",
    "            self.seeds.sort()\n",
    "            \n",
    "    def getSeeds(self):\n",
    "        return self.seeds\n",
    "    \n",
    "    def setPreviousTriple(self, pt):\n",
    "        self.previousTriple = pt\n",
    "        \n",
    "    def getPreviousTriple(self):\n",
    "        return self.previousTriple\n",
    "    \n",
    "    def updateSeeds(self, newSeeds):\n",
    "        for seed in newSeeds:\n",
    "            self.setSeeds(seed)\n",
    "            \n",
    "    def setSubject(self, sub):\n",
    "        self.subject = sub\n",
    "    def setObject(self, obj):\n",
    "        self.object = obj\n",
    "    def setPredicate(self, pre):\n",
    "        self.predicate = pre\n",
    "    def setCost(self, c):\n",
    "        self.cost = c\n",
    "    def getCost(self):\n",
    "        return self.cost\n",
    "    def getSubject(self):\n",
    "        return self.subject\n",
    "    def getObject(self):\n",
    "        return self.object\n",
    "    def getPredicate(self):\n",
    "        return self.predicate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Expand Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Old version of expand\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "def expand(queryTriples, eg, di, predicatesToMatch, wemb):\n",
    "    newQueryTriples = []\n",
    "    matchingTriples = []\n",
    "    \n",
    "    tmp = []\n",
    "    print(\"queryTriple size : \" + str(len(queryTriples)))\n",
    "    for queryObject in queryTriples:\n",
    "        queryString = makeQueryString(queryObject.getObject(), filterStringPredicates,filterStringSubjects, literalsToConnect)\n",
    "        print(\"----- Current Query ------\")\n",
    "        print(queryString)\n",
    "        print(\"--------------------------\")\n",
    "        \n",
    "        sparql.setQuery(queryString)\n",
    "        sparql.setReturnFormat(JSON)\n",
    "        results = sparql.query().convert()\n",
    "\n",
    "        for result in results[\"results\"][\"bindings\"]:\n",
    "            if 's' in result:\n",
    "                if(isEntityInDatabase(result[\"s\"][\"value\"])):\n",
    "                    #print(result[\"s\"][\"value\"] + \"   ---   \" + result[\"p\"][\"value\"])\n",
    "                    newTriple = Triple()\n",
    "                    newTriple.setSubject(queryObject.getObject())\n",
    "                    newTriple.setPredicate(result[\"p\"][\"value\"])\n",
    "                    newTriple.setObject(result[\"s\"][\"value\"])\n",
    "                    #newTriple.setCost(computeCosineSimilarity(predicatesToMatch, result[\"p\"][\"value\"], wemb ))\n",
    "                    #newTriple.setCost(20)\n",
    "                    newTriple.setPreviousTriple(queryObject)\n",
    "                    \n",
    "                    for seed in queryObject.getSeeds():\n",
    "                        newTriple.setSeeds(seed)\n",
    "                    #print(newTriple)\n",
    "                    \n",
    "                    #Apply word embedding for costs and modify matchingTriples\n",
    "                    if newTriple not in matchingTriples:\n",
    "                        matchingTriples.append(newTriple)\n",
    "                    \n",
    "                    if newTriple not in expandedGraph:\n",
    "                        expandedGraph.append(newTriple)\n",
    "                    if newTriple.getObject() not in tmp:\n",
    "                        newQueryTriples.append(newTriple)\n",
    "                    \n",
    "                    #Check if it's in tmp, if yes, add triple into duplicatedItems\n",
    "                    if newTriple.getObject() in tmp:\n",
    "                        #print(\"add duplicated Items: \" + newTriple.getObject())\n",
    "                        addDuplicatedItems(newTriple, eg, di)\n",
    "                    else:\n",
    "                        tmp.append(newTriple.getObject())\n",
    "            else:\n",
    "                if(isEntityInDatabase(result[\"k\"][\"value\"])):\n",
    "                    #print(result[\"k\"][\"value\"] + \"   ---   \" + result[\"j\"][\"value\"])\n",
    "                    \n",
    "                    newTriple = Triple()\n",
    "                    newTriple.setSubject(queryObject.getObject())\n",
    "                    newTriple.setPredicate(result[\"j\"][\"value\"])                \n",
    "                    newTriple.setObject(result[\"k\"][\"value\"])\n",
    "                    #newTriple.setCost(20)\n",
    "                    #newTriple.setCost(computeCosineSimilarity(predicatesToMatch, result[\"j\"][\"value\"], wemb ))\n",
    "                    newTriple.setPreviousTriple(queryObject)\n",
    "                    \n",
    "                    for seed in queryObject.getSeeds():\n",
    "                        newTriple.setSeeds(seed)\n",
    "                    \n",
    "                    #print(newTriple)\n",
    "                    \n",
    "                    #Apply word embedding for costs and modify matchingTriples\n",
    "                    if newTriple not in matchingTriples:\n",
    "                        matchingTriples.append(newTriple)\n",
    "                    \n",
    "                    if newTriple not in expandedGraph:\n",
    "                        expandedGraph.append(newTriple)\n",
    "                    if newTriple.getObject() not in tmp:\n",
    "                        newQueryTriples.append(newTriple)\n",
    "                        \n",
    "                    #Check if it's in tmp, if yes, add triple into duplicatedItems\n",
    "                    if newTriple.getObject() in tmp:\n",
    "                        #print(\"add duplicated Items: \" + newTriple.getObject())\n",
    "                        addDuplicatedItems(newTriple, eg, di)\n",
    "                    else:\n",
    "                        tmp.append(newTriple.getObject())\n",
    "        computeCosts(predicatesToMatch, matchingTriples, wemb)\n",
    "    return newQueryTriples, matchingTriples\n",
    "'''\n",
    "print(\"Old version of expand\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand(queryTriples, eg, di, predicatesToMatch, wemb):\n",
    "    newQueryTriples = []\n",
    "    matchingTriples = []\n",
    "    \n",
    "    tmp = []\n",
    "    print(\"queryTriple size : \" + str(len(queryTriples)))\n",
    "    for queryObject in queryTriples:\n",
    "        queryString = makeQueryString(queryObject.getObject(), filterStringPredicates,filterStringSubjects, literalsToConnect)\n",
    "        print(\"----- Current Query ------\")\n",
    "        print(queryString)\n",
    "        print(\"--------------------------\")\n",
    "        \n",
    "        sparql.setQuery(queryString)\n",
    "        sparql.setReturnFormat(JSON)\n",
    "        results = sparql.query().convert()\n",
    "\n",
    "        for result in results[\"results\"][\"bindings\"]:\n",
    "            \n",
    "            if 's' in result:\n",
    "                newTriple = Triple()\n",
    "                newTriple.setSubject(queryObject.getObject())\n",
    "                newTriple.setPredicate(result[\"p\"][\"value\"])\n",
    "                newTriple.setObject(result[\"s\"][\"value\"])\n",
    "                newTriple.setPreviousTriple(queryObject)\n",
    "                    \n",
    "                for seed in queryObject.getSeeds():\n",
    "                    newTriple.setSeeds(seed)\n",
    "                    \n",
    "                if newTriple not in expandedGraph:\n",
    "                        expandedGraph.append(newTriple)\n",
    "                \n",
    "                #Apply word embedding for costs and modify matchingTriples\n",
    "                if newTriple not in matchingTriples:\n",
    "                    matchingTriples.append(newTriple)                       \n",
    "                \n",
    "                if(isEntityInDatabase(result[\"s\"][\"value\"])):                                   \n",
    "                    if newTriple.getObject() not in tmp:\n",
    "                        newQueryTriples.append(newTriple)\n",
    "                    \n",
    "                    #Check if it's in tmp, if yes, add triple into duplicatedItems\n",
    "                    if newTriple.getObject() in tmp:\n",
    "                        #print(\"add duplicated Items: \" + newTriple.getObject())\n",
    "                        addDuplicatedItems(newTriple, eg, di)\n",
    "                    else:\n",
    "                        tmp.append(newTriple.getObject())\n",
    "                        \n",
    "            else:\n",
    "                                    \n",
    "                newTriple = Triple()\n",
    "                newTriple.setSubject(queryObject.getObject())\n",
    "                newTriple.setPredicate(result[\"j\"][\"value\"])                \n",
    "                newTriple.setObject(result[\"k\"][\"value\"])\n",
    "                newTriple.setPreviousTriple(queryObject)\n",
    "\n",
    "                for seed in queryObject.getSeeds():\n",
    "                    newTriple.setSeeds(seed)\n",
    "\n",
    "                #Apply word embedding for costs and modify matchingTriples\n",
    "                if newTriple not in matchingTriples:\n",
    "                    matchingTriples.append(newTriple)\n",
    "\n",
    "                if newTriple not in expandedGraph:\n",
    "                    expandedGraph.append(newTriple)\n",
    "                    \n",
    "                if(isEntityInDatabase(result[\"k\"][\"value\"])):\n",
    "                    #print(result[\"k\"][\"value\"] + \"   ---   \" + result[\"j\"][\"value\"])\n",
    "\n",
    "                    if newTriple.getObject() not in tmp:\n",
    "                        newQueryTriples.append(newTriple)\n",
    "                        \n",
    "                    #Check if it's in tmp, if yes, add triple into duplicatedItems\n",
    "                    if newTriple.getObject() in tmp:\n",
    "                        #print(\"add duplicated Items: \" + newTriple.getObject())\n",
    "                        addDuplicatedItems(newTriple, eg, di)\n",
    "                    else:\n",
    "                        tmp.append(newTriple.getObject())\n",
    "        computeCosts(predicatesToMatch, matchingTriples, wemb)\n",
    "    return newQueryTriples, matchingTriples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# addDuplicatedItems Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addDuplicatedItems(ntp, eg, di):\n",
    "    for tp in eg:\n",
    "        if tp.getObject() == ntp.getObject() and tp not in di:\n",
    "            di.append(tp)\n",
    "            if tp.getSeeds() != ntp.getSeeds():\n",
    "                updateSeedsInExpandedGraph(tp,ntp,eg)           \n",
    "    #di.append(ntp)\n",
    "    return \"Dup\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def updateSeedsInExpandedGraph(tp,ntp,eg):\n",
    "    old1 = tp.getSeeds()[:]\n",
    "    old2 = ntp.getSeeds()[:]\n",
    "    ntp.updateSeeds(tp.getSeeds())\n",
    "    newS = ntp.getSeeds()\n",
    "    \n",
    "    for triple in eg:\n",
    "        if triple.getSeeds() == old1 or triple.getSeeds() == old2:\n",
    "            triple.updateSeeds(newS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reduction Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reductionTestsDegreeOne(eg):\n",
    "    \n",
    "    degreeOneNodes = []\n",
    "    \n",
    "    for tp1 in eg:\n",
    "        duplicatedObject = False\n",
    "        isLeafNode = True\n",
    "        for tp2 in eg:\n",
    "            if tp2.getPreviousTriple() == tp1:\n",
    "                isLeafNode = False\n",
    "            if tp1.getObject() == tp2.getObject() and not tp1.getSubject() == tp2.getSubject():\n",
    "                duplicatedObject = True\n",
    "                break\n",
    "        if not duplicatedObject and isLeafNode:\n",
    "            degreeOneNodes.append(tp1)\n",
    "    print(\"Degree One Nodes size: \" + str(len(degreeOneNodes)))\n",
    "    for tp in degreeOneNodes:\n",
    "        eg.remove(tp)\n",
    "\n",
    "#TODO: Fix Bug\n",
    "def keepMinEdge(eg):\n",
    "    \n",
    "    triplesToBeRemoved = []\n",
    "    \n",
    "    for tp1 in eg:\n",
    "        minTriple = tp1\n",
    "        for tp2 in eg:\n",
    "            if tp1.getSubject() == tp2.getSubject() and tp1.getObject() == tp2.getObject and not tp1.getPredicate() == tp2.getPredicate():\n",
    "                if tp2.getCost() < tp1.getCost():\n",
    "                    minTriple = tp2\n",
    "                    if tp1 not in triplesToBeRemoved:\n",
    "                        triplesToBeRemoved.append(tp1)\n",
    "                else:\n",
    "                    if tp2 not in triplesToBeRemoved:\n",
    "                        triplesToBeRemoved.append(tp2)\n",
    "    for tp in triplesToBeRemoved:\n",
    "        eg.remove(tp)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# checkConnection Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkConnection(ltc, di):\n",
    "    if len(ltc) == 1:\n",
    "        return False\n",
    "    else:\n",
    "        ltc.sort()\n",
    "        for tp in di:\n",
    "            if tp.getSeeds() == ltc:\n",
    "                return True\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ComputeCosts Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeCosts(predicatesToMatch, matchingTriples, wemb):\n",
    "    th = 30\n",
    "    predicateList = []\n",
    "    \n",
    "    for tp in matchingTriples:\n",
    "        predicate = tp.getPredicate()\n",
    "        predicateList.append(predicate[predicate.rindex('/')+1:])\n",
    "        \n",
    "    for y in predicatesToMatch:\n",
    "        ar = []\n",
    "        for p in predicateList:\n",
    "            ar.append([y, p])\n",
    "        ar = np.array(ar)\n",
    "        result = evaluate_similarity_score(wemb, ar, 'xxx')\n",
    "        #print(\"size of result :\" + str(len(result)) + '  ----  size of matching Triples : ' + str(len(matchingTriples)))\n",
    "        for x in range(len(result)):\n",
    "            #when two comparing predicates are both not in the word embedding, then using JW distance instead\n",
    "            #TODO: What if one of them is in word embedding?\n",
    "            if result[x] == 1.0 and y != predicateList[x]:\n",
    "                #print(y + \"  ---  \" + predicateList[x])\n",
    "                result[x] = dw(y, predicateList[x])\n",
    "            elif (1 - result[x]) * 100 > th:\n",
    "                result[x] = dw(y, predicateList[x])\n",
    "                \n",
    "            if matchingTriples[x].getCost() is None:\n",
    "                matchingTriples[x].setCost((1 - result[x]) * 100)\n",
    "            elif matchingTriples[x].getCost() > (1 - result[x]) * 100:\n",
    "                matchingTriples[x].setCost((1 - result[x]) * 100)\n",
    "    \n",
    "    index = 0\n",
    "    \n",
    "    while index < len(matchingTriples):\n",
    "        if matchingTriples[index].getCost() > th:\n",
    "            matchingTriples.pop(index)\n",
    "        else:\n",
    "            index += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeCosts2(predicatesToMatch, matchingTriples, wemb):\n",
    "    th = 30\n",
    "    predicateList = []\n",
    "    \n",
    "    for tp in matchingTriples:\n",
    "        predicate = tp.getPredicate()\n",
    "        predicateList.append(predicate[predicate.rindex('/')+1:])\n",
    "        \n",
    "    for y in predicatesToMatch:\n",
    "        ar = []\n",
    "        for p in predicateList:\n",
    "            ar.append([y, p])\n",
    "        ar = np.array(ar)\n",
    "        result = evaluate_similarity_score(wemb, ar, 'xxx')\n",
    "        #print(\"size of result :\" + str(len(result)) + '  ----  size of matching Triples : ' + str(len(matchingTriples)))\n",
    "        for x in range(len(result)):\n",
    "            #when two comparing predicates are both not in the word embedding, then using JW distance instead\n",
    "            #TODO: What if one of them is in word embedding?\n",
    "            if result[x] == 1.0 and y != predicateList[x]:\n",
    "                #print(y + \"  ---  \" + predicateList[x])\n",
    "                result[x] = dw(y, predicateList[x])\n",
    "            elif (1 - result[x]) * 100 > th:\n",
    "                result[x] = dw(y, predicateList[x])\n",
    "                \n",
    "            if matchingTriples[x].getCost() is None:\n",
    "                matchingTriples[x].setCost((1 - result[x]) * 100)\n",
    "            elif matchingTriples[x].getCost() > (1 - result[x]) * 100:\n",
    "                matchingTriples[x].setCost((1 - result[x]) * 100)\n",
    "    \n",
    "    index = 0\n",
    "    \n",
    "    while index < len(matchingTriples):\n",
    "        if matchingTriples[index].getCost() > th:\n",
    "            matchingTriples.pop(index)\n",
    "        else:\n",
    "            index += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identify Entities and predicates in keywords\n",
    "###### 1. check if the lowercased literal is a property in dataset. If yes, then it is a predicate.\n",
    "###### 2. check if the convertFirstToCapital literal is a property in dataset. If yes, then it is a predicate.\n",
    "###### 3. check if the convertFirstToCapital literal is a resource in dataset. If yes, then it is an entity\n",
    "###### 4. Treat every literal as a predicate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isResourceInDataset2(literal):\n",
    "    jw_th = 0.7\n",
    "    \n",
    "    queryString = 'select distinct ?s where {?s ?p \"'+ literal.lower() + '\"@en}'\n",
    "    sparql.setQuery(queryString)\n",
    "    sparql.setReturnFormat(JSON)\n",
    "    results = sparql.query().convert()\n",
    "    \n",
    "    for result in results[\"results\"][\"bindings\"]:\n",
    "        #print(result[\"s\"][\"value\"])\n",
    "        sub = result[\"s\"][\"value\"]\n",
    "        if 'property' in sub:\n",
    "            if dw(sub[sub.rindex('/')+1:],literal) > jw_th:\n",
    "                return False\n",
    "    \n",
    "    literal = convertFirstToCapital(literal)\n",
    "\n",
    "    queryString = 'select distinct ?s where {?s ?p \"'+ literal + '\"@en}'\n",
    "    sparql.setQuery(queryString)\n",
    "    sparql.setReturnFormat(JSON)\n",
    "    results = sparql.query().convert()\n",
    "    \n",
    "    propertyInSub = False\n",
    "    resourceInSub = False\n",
    "    for result in results[\"results\"][\"bindings\"]:\n",
    "        #print(result[\"s\"][\"value\"])\n",
    "        sub = result[\"s\"][\"value\"]\n",
    "        try:\n",
    "            p = dw(sub[sub.rindex('/') +1:],literal)\n",
    "        except:\n",
    "            print(\"Substring not found, treat as a property\")\n",
    "            return False\n",
    "        \n",
    "        if 'property' in sub:\n",
    "            if  p > jw_th:\n",
    "                propertyInSub = True\n",
    "        elif 'resource' in sub:\n",
    "            if p > jw_th:\n",
    "                resourceInSub = True\n",
    "                \n",
    "    if propertyInSub:\n",
    "        return False\n",
    "    elif resourceInSub:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isResourceInDataset(literal):\n",
    "    jw_th = 0.7\n",
    "    \n",
    "    #Original keyword:\n",
    "    queryString = 'select distinct ?s where {?s ?p \"'+ literal + '\"@en}'\n",
    "    sparql.setQuery(queryString)\n",
    "    sparql.setReturnFormat(JSON)\n",
    "    results = sparql.query().convert()\n",
    "    \n",
    "    for result in results[\"results\"][\"bindings\"]:\n",
    "        #print(result[\"s\"][\"value\"])\n",
    "        sub = result[\"s\"][\"value\"]\n",
    "        #print(sub)\n",
    "        try:\n",
    "            p = dw(sub[sub.rindex('/') +1:],literal)\n",
    "        except:\n",
    "            print(\"Find a record with no / in lowercased\")\n",
    "            \n",
    "        if 'resource' in sub and p > jw_th:\n",
    "            return True\n",
    "        \n",
    "    #Lowercase:\n",
    "    queryString = 'select distinct ?s where {?s ?p \"'+ literal.lower() + '\"@en}'\n",
    "    sparql.setQuery(queryString)\n",
    "    sparql.setReturnFormat(JSON)\n",
    "    results = sparql.query().convert()\n",
    "    \n",
    "    for result in results[\"results\"][\"bindings\"]:\n",
    "        #print(result[\"s\"][\"value\"])\n",
    "        sub = result[\"s\"][\"value\"]\n",
    "        #print(sub)\n",
    "        try:\n",
    "            p = dw(sub[sub.rindex('/') +1:],literal)\n",
    "        except:\n",
    "            print(\"Find a record with no / in lowercased\")\n",
    "            \n",
    "        if 'resource' in sub and p > jw_th:\n",
    "            return True\n",
    "    \n",
    "    #First letter capitalized\n",
    "    literal = convertFirstToCapital(literal)\n",
    "    \n",
    "    queryString = 'select distinct ?s where {?s ?p \"'+ literal + '\"@en}'\n",
    "    sparql.setQuery(queryString)\n",
    "    sparql.setReturnFormat(JSON)\n",
    "    results = sparql.query().convert()\n",
    "    \n",
    "    for result in results[\"results\"][\"bindings\"]:\n",
    "        #print(result[\"s\"][\"value\"])\n",
    "        sub = result[\"s\"][\"value\"]\n",
    "        #print(sub+ \" cftc\")\n",
    "        try:\n",
    "            p = dw(sub[sub.rindex('/') +1:],literal)\n",
    "        except:\n",
    "            print(\"Find a record with no / in CFTC\")\n",
    "        \n",
    "        if 'resource' in sub and p > jw_th:\n",
    "            return True\n",
    "                \n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def indentifyEntitiesAndPredicates(keywords):\n",
    "    entity = []\n",
    "    pre = []\n",
    "    \n",
    "    li = keywords.split(',')\n",
    "    \n",
    "    for x in range(len(li)):\n",
    "        li[x] = li[x].strip()\n",
    "        #print(\"current key word: \" + li[x])\n",
    "        if isResourceInDataset(li[x]):\n",
    "            entity.append(convertFirstToCapital(li[x]))\n",
    "        # we treat every keyword as a predicate\n",
    "        pre.append(li[x])\n",
    "    \n",
    "    for x in range(len(entity)):\n",
    "        entity[x] = '\"' + entity[x] + '\"@en'\n",
    "    return entity,pre\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jaro_Winkler Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make1shorter(word1, word2):\n",
    "    if len(word1) > len(word2):\n",
    "        # Make sure word1 is shorter\n",
    "        temp = word1\n",
    "        word1 = word2\n",
    "        word2 = temp\n",
    "\n",
    "    return word1, word2\n",
    "\n",
    "def dj(word1, word2):\n",
    "    if len(word1) == 0 or len(word2) == 0:\n",
    "        raise Exception(\"Not words, mate\")\n",
    "\n",
    "    word1, word2 = make1shorter(word1, word2)\n",
    "\n",
    "    word2chars = list(word2)\n",
    "    m = 0\n",
    "    for char in word1:\n",
    "        if char in word2chars:\n",
    "            m += 1\n",
    "            word2chars.pop(word2chars.index(char))\n",
    "\n",
    "    t = 0\n",
    "    for i in range(len(word1)):\n",
    "        if word1[i] != word2[i]:\n",
    "            t += 1\n",
    "\n",
    "    if m == 0:\n",
    "        return 0\n",
    "    return 1/3*(m/len(word1) + m/len(word2) + (m - t / 2)/m)\n",
    "\n",
    "def dw(word1, word2, p=0.1, lmax=4):\n",
    "    word1, word2 = make1shorter(word1, word2)\n",
    "\n",
    "    dj_ = dj(word1, word2)\n",
    "\n",
    "    l = 0\n",
    "    for i in range(min(len(word1), lmax)):\n",
    "        if word1[i] == word2[i]:\n",
    "            l += 1\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    return dj_ + l * p * (1 - dj_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert first letter in literals to capital case and lower case for the rest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertFirstToCapital(s):\n",
    "    res = s.split()\n",
    "    for x in range(len(res)):\n",
    "        res[x] = res[x][0].upper() + res[x][1:].lower()\n",
    "    res = \" \".join(res)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Find a record with no / in lowercased\n",
      "Find a record with no / in lowercased\n",
      "---- entities ----\n",
      "\"World Of Warcraft\"@en\n",
      "\"Develop\"@en\n",
      "['World of Warcraft', 'develop']\n",
      "queryTriple size : 2\n",
      "----- Current Query ------\n",
      "SELECT distinct ?s ?p WHERE { {?s ?p \"World Of Warcraft\"@en . } FILTER (!regex(str(?p), 'wikiPageWikiLink' , 'i') && !regex(str(?p), 'wikiPageRedirects' , 'i') && !regex(str(?p), 'wikiPageDisambiguates' , 'i') && !regex(str(?p), 'Thing' , 'i') && !regex(str(?p), 'wikiPageUsesTemplate' , 'i') && !regex(str(?p), 'rdf-syntax-ns#type' , 'i') && !regex(str(?s), 'entity' , 'i') && !regex(str(?s), 'Category' , 'i') && !regex(str(?s), 'wikidata' , 'i') && !regex(str(?s), 'owl#Thing' , 'i') && !regex(str(?s), 'http://wikidata.dbpedia.org/resource/Q' , 'i') )}\n",
      "--------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Missing 2 words. Will replace them with mean vector\n",
      "Missing 1 words. Will replace them with mean vector\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Current Query ------\n",
      "SELECT distinct ?s ?p WHERE { {?s ?p \"Develop\"@en . } FILTER (!regex(str(?p), 'wikiPageWikiLink' , 'i') && !regex(str(?p), 'wikiPageRedirects' , 'i') && !regex(str(?p), 'wikiPageDisambiguates' , 'i') && !regex(str(?p), 'Thing' , 'i') && !regex(str(?p), 'wikiPageUsesTemplate' , 'i') && !regex(str(?p), 'rdf-syntax-ns#type' , 'i') && !regex(str(?s), 'entity' , 'i') && !regex(str(?s), 'Category' , 'i') && !regex(str(?s), 'wikidata' , 'i') && !regex(str(?s), 'owl#Thing' , 'i') && !regex(str(?s), 'http://wikidata.dbpedia.org/resource/Q' , 'i') )}\n",
      "--------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Missing 4 words. Will replace them with mean vector\n",
      "Missing 1 words. Will replace them with mean vector\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "queryTriple size : 4\n",
      "----- Current Query ------\n",
      "SELECT distinct ?s ?p ?j ?k WHERE { {?s ?p <http://dbpedia.org/resource/World_Of_Warcraft> . FILTER (!regex(str(?p), 'wikiPageWikiLink' , 'i') && !regex(str(?p), 'wikiPageRedirects' , 'i') && !regex(str(?p), 'wikiPageDisambiguates' , 'i') && !regex(str(?p), 'Thing' , 'i') && !regex(str(?p), 'wikiPageUsesTemplate' , 'i') && !regex(str(?p), 'rdf-syntax-ns#type' , 'i') && !regex(str(?s), 'entity' , 'i') && !regex(str(?s), 'Category' , 'i') && !regex(str(?s), 'wikidata' , 'i') && !regex(str(?s), 'owl#Thing' , 'i') && !regex(str(?s), 'http://wikidata.dbpedia.org/resource/Q' , 'i') )} UNION {<http://dbpedia.org/resource/World_Of_Warcraft> ?j ?k. FILTER (!regex(str(?j), 'wikiPageWikiLink' , 'i') && !regex(str(?j), 'wikiPageRedirects' , 'i') && !regex(str(?j), 'wikiPageDisambiguates' , 'i') && !regex(str(?j), 'Thing' , 'i') && !regex(str(?j), 'wikiPageUsesTemplate' , 'i') && !regex(str(?j), 'rdf-syntax-ns#type' , 'i') && !regex(str(?k), 'entity' , 'i') && !regex(str(?k), 'Category' , 'i') && !regex(str(?k), 'wikidata' , 'i') && !regex(str(?k), 'owl#Thing' , 'i') && !regex(str(?k), 'http://wikidata.dbpedia.org/resource/Q' , 'i') )}}\n",
      "--------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Missing 14 words. Will replace them with mean vector\n",
      "Missing 7 words. Will replace them with mean vector\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Current Query ------\n",
      "SELECT distinct ?s ?p ?j ?k WHERE { {?s ?p <http://dbpedia.org/resource/Develop> . FILTER (!regex(str(?p), 'wikiPageWikiLink' , 'i') && !regex(str(?p), 'wikiPageRedirects' , 'i') && !regex(str(?p), 'wikiPageDisambiguates' , 'i') && !regex(str(?p), 'Thing' , 'i') && !regex(str(?p), 'wikiPageUsesTemplate' , 'i') && !regex(str(?p), 'rdf-syntax-ns#type' , 'i') && !regex(str(?s), 'entity' , 'i') && !regex(str(?s), 'Category' , 'i') && !regex(str(?s), 'wikidata' , 'i') && !regex(str(?s), 'owl#Thing' , 'i') && !regex(str(?s), 'http://wikidata.dbpedia.org/resource/Q' , 'i') )} UNION {<http://dbpedia.org/resource/Develop> ?j ?k. FILTER (!regex(str(?j), 'wikiPageWikiLink' , 'i') && !regex(str(?j), 'wikiPageRedirects' , 'i') && !regex(str(?j), 'wikiPageDisambiguates' , 'i') && !regex(str(?j), 'Thing' , 'i') && !regex(str(?j), 'wikiPageUsesTemplate' , 'i') && !regex(str(?j), 'rdf-syntax-ns#type' , 'i') && !regex(str(?k), 'entity' , 'i') && !regex(str(?k), 'Category' , 'i') && !regex(str(?k), 'wikidata' , 'i') && !regex(str(?k), 'owl#Thing' , 'i') && !regex(str(?k), 'http://wikidata.dbpedia.org/resource/Q' , 'i') )}}\n",
      "--------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Missing 14 words. Will replace them with mean vector\n",
      "Missing 7 words. Will replace them with mean vector\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Current Query ------\n",
      "SELECT distinct ?s ?p ?j ?k WHERE { {?s ?p <http://dbpedia.org/resource/Develop_(magazine)> . FILTER (!regex(str(?p), 'wikiPageWikiLink' , 'i') && !regex(str(?p), 'wikiPageRedirects' , 'i') && !regex(str(?p), 'wikiPageDisambiguates' , 'i') && !regex(str(?p), 'Thing' , 'i') && !regex(str(?p), 'wikiPageUsesTemplate' , 'i') && !regex(str(?p), 'rdf-syntax-ns#type' , 'i') && !regex(str(?s), 'entity' , 'i') && !regex(str(?s), 'Category' , 'i') && !regex(str(?s), 'wikidata' , 'i') && !regex(str(?s), 'owl#Thing' , 'i') && !regex(str(?s), 'http://wikidata.dbpedia.org/resource/Q' , 'i') )} UNION {<http://dbpedia.org/resource/Develop_(magazine)> ?j ?k. FILTER (!regex(str(?j), 'wikiPageWikiLink' , 'i') && !regex(str(?j), 'wikiPageRedirects' , 'i') && !regex(str(?j), 'wikiPageDisambiguates' , 'i') && !regex(str(?j), 'Thing' , 'i') && !regex(str(?j), 'wikiPageUsesTemplate' , 'i') && !regex(str(?j), 'rdf-syntax-ns#type' , 'i') && !regex(str(?k), 'entity' , 'i') && !regex(str(?k), 'Category' , 'i') && !regex(str(?k), 'wikidata' , 'i') && !regex(str(?k), 'owl#Thing' , 'i') && !regex(str(?k), 'http://wikidata.dbpedia.org/resource/Q' , 'i') )}}\n",
      "--------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Missing 62 words. Will replace them with mean vector\n",
      "Missing 23 words. Will replace them with mean vector\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Current Query ------\n",
      "SELECT distinct ?s ?p ?j ?k WHERE { {?s ?p <http://dbpedia.org/resource/DVLP> . FILTER (!regex(str(?p), 'wikiPageWikiLink' , 'i') && !regex(str(?p), 'wikiPageRedirects' , 'i') && !regex(str(?p), 'wikiPageDisambiguates' , 'i') && !regex(str(?p), 'Thing' , 'i') && !regex(str(?p), 'wikiPageUsesTemplate' , 'i') && !regex(str(?p), 'rdf-syntax-ns#type' , 'i') && !regex(str(?s), 'entity' , 'i') && !regex(str(?s), 'Category' , 'i') && !regex(str(?s), 'wikidata' , 'i') && !regex(str(?s), 'owl#Thing' , 'i') && !regex(str(?s), 'http://wikidata.dbpedia.org/resource/Q' , 'i') )} UNION {<http://dbpedia.org/resource/DVLP> ?j ?k. FILTER (!regex(str(?j), 'wikiPageWikiLink' , 'i') && !regex(str(?j), 'wikiPageRedirects' , 'i') && !regex(str(?j), 'wikiPageDisambiguates' , 'i') && !regex(str(?j), 'Thing' , 'i') && !regex(str(?j), 'wikiPageUsesTemplate' , 'i') && !regex(str(?j), 'rdf-syntax-ns#type' , 'i') && !regex(str(?k), 'entity' , 'i') && !regex(str(?k), 'Category' , 'i') && !regex(str(?k), 'wikidata' , 'i') && !regex(str(?k), 'owl#Thing' , 'i') && !regex(str(?k), 'http://wikidata.dbpedia.org/resource/Q' , 'i') )}}\n",
      "--------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Missing 102 words. Will replace them with mean vector\n",
      "Missing 27 words. Will replace them with mean vector\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matching Triples: -----   size: 0\n",
      "Wall time: 4.64 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "with open('qald5-3.txt','r',encoding='utf-8') as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "with open('test_res-3-2.txt','w',encoding='utf-8') as fw:\n",
    "    for line in lines:\n",
    "        filterStringPredicates = [\"wikiPageWikiLink\",\"wikiPageRedirects\",\"wikiPageDisambiguates\", \"Thing\",\"wikiPageUsesTemplate\",\"rdf-syntax-ns#type\"]\n",
    "        filterStringSubjects = [\"entity\", \"Category\", \"wikidata\",\"owl#Thing\", \"http://wikidata.dbpedia.org/resource/Q\"]\n",
    "        expandedGraph = []\n",
    "        duplicatedItems = []\n",
    "\n",
    "        literalsToConnect, predicates = indentifyEntitiesAndPredicates(line.strip())\n",
    "        fw.write(\"-----Question------\\n\")\n",
    "        fw.write(line + '\\n')\n",
    "        fw.write(\"-------------------\\n\")\n",
    "        qtps = []\n",
    "        for literal in literalsToConnect:\n",
    "            triple = Triple()\n",
    "            triple.setObject(literal)\n",
    "            triple.setSeeds(literal)\n",
    "            qtps.append(triple)\n",
    "\n",
    "        fw.write('--entities--\\n')\n",
    "        print('---- entities ----')\n",
    "        for tp in qtps:\n",
    "            fw.write(str(tp.getObject()) + '\\n')\n",
    "            print(tp.getObject())\n",
    "        print(predicates)\n",
    "        # Test Purpose\n",
    "        #\"\"\"\n",
    "        levelOfExpansion = 1\n",
    "        while(levelOfExpansion < 3 and not checkConnection(literalsToConnect, duplicatedItems)):\n",
    "            #add mtps to a new list\n",
    "            qtps, mtps = expand(qtps, expandedGraph, duplicatedItems, predicates, we)\n",
    "            levelOfExpansion += 1\n",
    "\n",
    "        print(\"Matching Triples: -----   size: \" + str(len(mtps)))\n",
    "        mtps.sort()\n",
    "        for tp in mtps:\n",
    "            fw.write(str(tp) + '\\n')\n",
    "            print(tp)\n",
    "        if checkConnection(literalsToConnect, duplicatedItems):\n",
    "            reductionTestsDegreeOne(expandedGraph)\n",
    "            fw.write(\"Size of expanded graph: \"+ str(len(expandedGraph)) + '\\n')\n",
    "            for tp in expandedGraph:\n",
    "                fw.write(str(tp) + '\\n')\n",
    "        \n",
    "        #for x in expandedGraph:\n",
    "        #    print(x)\n",
    "#                \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "levelOfExpansion = 1\n",
    "while(levelOfExpansion < 3 or not checkConnection(literalsToConnect, duplicatedItems)):\n",
    "    #add mtps to a new list\n",
    "    qtps, mtps = expand(qtps, expandedGraph, duplicatedItems, predicates, we)\n",
    "    levelOfExpansion += 1\n",
    "\n",
    "print(\"Matching Triples: -----   size: \" + str(len(mtps)))\n",
    "for tp in mtps:\n",
    "    print(tp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Size of expanded graph: \"+ str(len(expandedGraph)))\n",
    "for tp in expandedGraph:\n",
    "    print(tp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of duplicatedItems: 4\n",
      "http://dbpedia.org/resource/Hells_Angels  --  http://xmlns.com/foaf/0.1/primaryTopic  --  http://en.wikipedia.org/wiki/Hells_Angels  [\"Hells Angels\"@en] (70.74291408061981)\n",
      "http://dbpedia.org/resource/Hells_Angels  --  http://xmlns.com/foaf/0.1/isPrimaryTopicOf  --  http://en.wikipedia.org/wiki/Hells_Angels  [\"Hells Angels\"@en] (70.74291408061981)\n",
      "http://dbpedia.org/resource/Hells_Angels  --  http://dbpedia.org/ontology/knownFor  --  http://dbpedia.org/resource/Sonny_Barger  [\"Hells Angels\"@en] (70.74291408061981)\n",
      "http://dbpedia.org/resource/Hells_Angels  --  http://dbpedia.org/property/keyPeople  --  http://dbpedia.org/resource/Sonny_Barger  [\"Hells Angels\"@en] (70.74291408061981)\n"
     ]
    }
   ],
   "source": [
    "print(\"Size of duplicatedItems: \"+ str(len(duplicatedItems)))\n",
    "for tp in duplicatedItems:\n",
    "    print(tp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reductionTestsDegreeOne(expandedGraph)\n",
    "print(\"Size of expanded graph: \"+ str(len(expandedGraph)))\n",
    "for tp in expandedGraph:\n",
    "    print(tp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of expanded graph: 0\n"
     ]
    }
   ],
   "source": [
    "keepMinEdge(expandedGraph)\n",
    "print(\"Size of expanded graph: \"+ str(len(expandedGraph)))\n",
    "for tp in expandedGraph:\n",
    "    print(tp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Purpose -- Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5351437\n"
     ]
    }
   ],
   "source": [
    "k = np.array([['mayor', 'city']])\n",
    "result = evaluate_similarity_score(wecn, k, 'xxx')\n",
    "print(result[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "w1 = 'admit'\n",
    "w2 = 'admit'\n",
    "print(dw(w1,w2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print('1' != '2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9555555555555555"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dw('develop','developer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filterStringPredicates = [\"wikiPageWikiLink\",\"wikiPageRedirects\",\"wikiPageDisambiguates\", \"Thing\",\"wikiPageUsesTemplate\",\"rdf-syntax-ns#type\"]\n",
    "filterStringSubjects = [\"entity\", \"Category\", \"wikidata\",\"owl#Thing\", \"http://wikidata.dbpedia.org/resource/Q\"]\n",
    "expandedGraph = []\n",
    "duplicatedItems = []\n",
    "line = 'Alberta, admit, province'\n",
    "literalsToConnect, predicates = indentifyEntitiesAndPredicates(line.strip())\n",
    "qtps = []\n",
    "for literal in literalsToConnect:\n",
    "    triple = Triple()\n",
    "    triple.setObject(literal)\n",
    "    triple.setSeeds(literal)\n",
    "    qtps.append(triple)\n",
    "\n",
    "for tp in qtps:\n",
    "    print(tp.getObject())\n",
    "\n",
    "# Test Purpose\n",
    "\n",
    "levelOfExpansion = 1\n",
    "while(levelOfExpansion < 3 and not checkConnection(literalsToConnect, duplicatedItems)):\n",
    "    #add mtps to a new list\n",
    "    qtps, mtps = expand(qtps, expandedGraph, duplicatedItems, predicates, we)\n",
    "    levelOfExpansion += 1\n",
    "\n",
    "print(\"Matching Triples: -----   size: \" + str(len(mtps)))\n",
    "for tp in mtps:\n",
    "    print(tp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(isResourceInDataset(\"wrote\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "queryString = 'select distinct ?s ?p where {<http://dbpedia.org/resource/The_Hunger_Games> ?p ?s}'\n",
    "sparql.setQuery(queryString)\n",
    "sparql.setReturnFormat(JSON)\n",
    "results = sparql.query().convert()\n",
    "\n",
    "for result in results[\"results\"][\"bindings\"]:\n",
    "    print(result[\"p\"][\"value\"] + \" --- \" +result[\"s\"][\"value\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "substring not found",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-27-d1c63f02909c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"jink\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrindex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'm'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m: substring not found"
     ]
    }
   ],
   "source": [
    "print(\"jink\".rindex('m'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
